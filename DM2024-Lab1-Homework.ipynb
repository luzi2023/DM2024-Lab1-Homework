{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: 盧子涵\n",
    "\n",
    "Student ID: 113065542\n",
    "\n",
    "GitHub ID: luzi2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: do the **take home** exercises in the [DM2024-Lab1-Master](https://github.com/didiersalazar/DM2024-Lab1-Master.git). You may need to copy some cells from the Lab notebook to this notebook. __This part is worth 20% of your grade.__\n",
    "\n",
    "\n",
    "2. Second: follow the same process from the [DM2024-Lab1-Master](https://github.com/didiersalazar/DM2024-Lab1-Master.git) on **the new dataset**. You don't need to explain all details as we did (some **minimal comments** explaining your code are useful though).  __This part is worth 30% of your grade.__\n",
    "    - Download the [the new dataset](https://huggingface.co/datasets/Senem/Nostalgic_Sentiment_Analysis_of_YouTube_Comments_Data). The dataset contains a `sentiment` and `comment` columns, with the sentiment labels being: 'nostalgia' and 'not nostalgia'. Read the specificiations of the dataset for background details. \n",
    "    - You are allowed to use and modify the `helper` functions in the folder of the first lab session (notice they may need modification) or create your own.\n",
    "\n",
    "\n",
    "3. Third: please attempt the following tasks on **the new dataset**. __This part is worth 30% of your grade.__\n",
    "    - Generate meaningful **new data visualizations**. Refer to online resources and the Data Mining textbook for inspiration and ideas. \n",
    "    - Generate **TF-IDF features** from the tokens of each text. This will generating a document matrix, however, the weights will be computed differently (using the TF-IDF value of each word per document as opposed to the word frequency). Refer to this Scikit-learn [guide](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) .\n",
    "    - Implement a simple **Naive Bayes classifier** that automatically classifies the records into their categories. Use both the TF-IDF features and word frequency features to build two seperate classifiers. Note that for the TF-IDF features you might need to use other type of NB classifier different than the one in the Master Notebook. Comment on the differences.  Refer to this [article](https://hub.packtpub.com/implementing-3-naive-bayes-classifiers-in-scikit-learn/).\n",
    "\n",
    "\n",
    "4. Fourth: In the lab, we applied each step really quickly just to illustrate how to work with your dataset. There are somethings that are not ideal or the most efficient/meaningful. Each dataset can be handled differently as well. What are those inefficent parts you noticed? How can you improve the Data preprocessing for these specific datasets? __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "5. Fifth: It's hard for us to follow if your code is messy, so please **tidy up your notebook** and **add minimal comments where needed**. __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "You can submit your homework following these guidelines: [Git Intro & How to hand your homework](https://github.com/didiersalazar/DM2024-Lab1-Master/blob/main/Git%20Intro%20%26%20How%20to%20hand%20your%20homework.ipynb). Make sure to commit and save your changes to your repository __BEFORE the deadline (October 27th 11:59 pm, Sunday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **=========First Part========================================**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **>>> Exercise 2 (take home):** \n",
    "Experiment with other querying techniques using pandas dataframes. Refer to their [documentation](https://pandas.pydata.org/pandas-docs/stable/indexing.html) for more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer here\n",
    "X.where(X['category_name'] == 'comp.graphics').dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 5 (take home)** \n",
    "There is an old saying that goes, \"The devil is in the details.\" When we are working with extremely large data, it's difficult to check records one by one (as we have been doing so far). And also, we don't even know what kind of missing values we are facing. Thus, \"debugging\" skills get sharper as we spend more time solving bugs. Let's focus on a different method to check for missing values and the kinds of missing values you may encounter. It's not easy to check for missing values as you will find out in a minute.\n",
    "\n",
    "Please check the data and the process below, describe what you observe and why it happened.   \n",
    "$Hint$ :  why `.isnull()` didn't work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "NA_dict = [{ 'id': 'A', 'missing_example': np.nan },\n",
    "           { 'id': 'B'                    },\n",
    "           { 'id': 'C', 'missing_example': 'NaN'  },\n",
    "           { 'id': 'D', 'missing_example': 'None' },\n",
    "           { 'id': 'E', 'missing_example':  None  },\n",
    "           { 'id': 'F', 'missing_example': ''     }]\n",
    "\n",
    "NA_df = pd.DataFrame(NA_dict, columns = ['id','missing_example'])\n",
    "NA_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Exercise 6 (take home):\n",
    "Notice any changes from the `X` dataframe to the `X_sample` dataframe? What are they? Report every change you noticed as compared to the previous state of `X`. Feel free to query and look more closely at the dataframe for these changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "# X is in sorted index order while X_sample isn't\n",
    "# the proportion of each category still remain the same\n",
    "X_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 8 (take home):** \n",
    "We can also do a side-by-side comparison of the distribution between the two datasets, but maybe you can try that as an excerise. Below we show you an snapshot of the type of chart we are looking for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "import numpy as np\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "x_cate_name = list(X['category_name'].value_counts())\n",
    "sample_cate_name = list(X_sample['category_name'].value_counts())\n",
    "plt.bar(np.arange(len(categories)) - 0.1, x_cate_name, width = 0.2)\n",
    "plt.bar(np.arange(len(categories)) + 0.1, sample_cate_name, width = 0.2)\n",
    "\n",
    "plt.xticks([0, 1, 2, 3], list(categories))\n",
    "plt.title('Category distribution')\n",
    "plt.legend(['category_name', 'category_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **>>> Exercise 10 (take home):**\n",
    "We said that the `1` at the beginning of the fifth record represents the `00` term. Notice that there is another 1 in the same record. Can you provide code that can verify what word this 1 represents from the vocabulary. Try to do this as efficient as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "X_counts[4, 0:100] # we can see that 1 exists in position 0 and 37\n",
    "count_vect.get_feature_names_out()[37:38]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **>>> Exercise 11 (take home):** \n",
    "From the chart above, we can see how sparse the term-document matrix is; i.e., there is only one terms with **FREQUENCY** of `1` in the subselection of the matrix. By the way, you may have noticed that we only selected 20 articles and 20 terms to plot the histrogram. As an excersise you can try to modify the code above to plot the entire term-document matrix or just a sample of it. How would you do this efficiently? Remember there is a lot of words in the vocab. Report below what methods you would use to get a nice and useful visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only the vector that total exist frequency >= 400\n",
    "# then we can observe the most significant words we have\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "sum_freq = []\n",
    "for i in trange(X_counts.shape[1]):\n",
    "    cnt = 0\n",
    "    for j in range(X_counts.shape[0]):\n",
    "        if str(X_counts[j, i]) == '1':\n",
    "            cnt += 1\n",
    "    sum_freq.append(cnt)\n",
    "# print(sum_freq)\n",
    "remove_term_index = []\n",
    "new_vect = []\n",
    "for i in trange(len(sum_freq)):\n",
    "    if sum_freq[i] <= 400:\n",
    "        remove_term_index.append(i)\n",
    "    else:\n",
    "        new_vect.append(count_vect.get_feature_names_out()[i])\n",
    "# print(len(remove_term_index))\n",
    "# print(len(new_vect))\n",
    "new_X_counts = []\n",
    "for i in trange(X_counts.shape[0]):\n",
    "    docs = []\n",
    "    for j in range(X_counts.shape[1]):\n",
    "        if j in remove_term_index:\n",
    "            continue\n",
    "        else:\n",
    "            docs.append(X_counts[i, j])\n",
    "    new_X_counts.append(docs)\n",
    "# print(len(new_X_counts[0, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Answer here\n",
    "plot_x = [\"term_\"+str(i) for i in new_vect]\n",
    "plot_y = [\"doc_\"+str(i) for i in list(X.index)[0:20]]\n",
    "plot_z = new_X_counts[:20]\n",
    "df_todraw = pd.DataFrame(plot_z, columns = plot_x, index = plot_y)\n",
    "plt.subplots(figsize=(19, 7))\n",
    "ax = sns.heatmap(df_todraw,\n",
    "                 cmap=\"PuRd\",\n",
    "                 vmin=0, vmax=1, annot=True)\n",
    "# print(X_counts.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 12 (take home):**\n",
    "If you want a nicer interactive visualization here, I would encourage you try to install and use plotly to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 13 (take home):** \n",
    "The chart above only contains 300 vocabulary in the documents, and it's already computationally intensive to both compute and visualize. Can you efficiently reduce the number of terms you want to visualize as an exercise. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "plt.subplots(figsize=(100, 10))\n",
    "# min_freq_threshold = 400\n",
    "# new_term_freq = []\n",
    "# new_count_vect = []\n",
    "# for i in trange(len(term_frequencies)):\n",
    "#     if term_frequencies[i] >= min_freq_threshold:\n",
    "#         new_term_freq.append(term_frequencies[i])\n",
    "#         new_count_vect.append(count_vect.get_feature_names_out()[i])\n",
    "# print(len(new_term_freq))\n",
    "# print(len(new_count_vect))\n",
    "g = sns.barplot(x=new_count_vect, \n",
    "            y=new_term_freq)\n",
    "_ = g.set_xticklabels(new_count_vect, rotation = 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 14 (take home):** \n",
    "Additionally, you can attempt to sort the terms on the `x-axis` by frequency instead of in alphabetical order. This way the visualization is more meaninfgul and you will be able to observe the so called [long tail](https://en.wikipedia.org/wiki/Long_tail) (get familiar with this term since it will appear a lot in data mining and other statistics courses). see picture below\n",
    "\n",
    "![alt txt](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Long_tail.svg/1000px-Long_tail.svg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "mapping = list(zip(new_count_vect, new_term_freq))\n",
    "sorted_list = sorted(mapping, key=lambda x: x[1], reverse=True)\n",
    "sorted_count_vect = []\n",
    "sorted_term_freq = []\n",
    "for vect, freq in sorted_list:\n",
    "    sorted_count_vect.append(vect)\n",
    "    sorted_term_freq.append(freq)\n",
    "g = sns.barplot(x=sorted_count_vect, \n",
    "            y=sorted_term_freq)\n",
    "# _ = g.set_xticklabels(sorted_count_vect, rotation = 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 15 (take home):** \n",
    "You can copy the code from the previous exercise and change the 'term_frequencies' variable for the 'term_frequencies_log', comment about the differences that you observe and talk about other possible insights that we can get from a log distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "plt.subplots(figsize=(100, 10))\n",
    "g = sns.barplot(x=count_vect.get_feature_names_out()[:300],\n",
    "                y=term_frequencies[:300])\n",
    "g.set_xticklabels(count_vect.get_feature_names_out()[:300], rotation = 90);\n",
    "# while doing the log transformation, we can decrease the effect of abnormal data to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### >>> **Exercise 16 (take home):** \n",
    "Review the words that were filtered in each category and comment about the differences and similarities that you can see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "def intersection(list1, list2):\n",
    "    return [value for value in list1 if value in list2]\n",
    "\n",
    "print(intersection(filtered_words_0, filtered_words_1))\n",
    "# the similarity is that in each category the words were filtered were stemmed-similar, such as 'criterion', 'critically', 'criticise', they are all the extension of 'critic'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 17 (take home):** \n",
    "Implement the FAE Top-K and MaxFPGrowth algorithms from the PAMI library to analyze the 'comp.graphics' category in our processed database. **Only implement the mining part of the algorithm and display the resulting patterns**, like we did with the FPGrowth algorithm after creating the new databases. For the FAE Top-K, run trials with k values of 500, 1000, and 1500, recording the runtime for each. For MaxFPGrowth, test minimum support thresholds of 3, 6, and 9, noting the runtime for these settings as well. Compare the patterns these algorithms extract with those from the previously implemented FPGrowth algorithm. Document your findings, focusing on differences and similarities in the outputs and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer Here\n",
    "from PAMI.frequentPattern.topk import FAE as alg\n",
    "\n",
    "trial_k = [500, 1000, 1500]\n",
    "for k in trial_k:\n",
    "    obj3 = alg.FAE(iFile='td_freq_db_comp_graphics.csv', k=k)    #initialize\n",
    "    obj3.mine()            #Start the mining process\n",
    "    frequentPatternsDF_comp_graphics= obj3.getPatternsAsDataFrame()\n",
    "\n",
    "    print(f\"top_{k}_term\")\n",
    "    print('Total No of patterns: ' + str(len(frequentPatternsDF_comp_graphics))) #print the total number of patterns\n",
    "    print('Runtime: ' + str(obj3.getRuntime())) #measure the runtime\n",
    "    \n",
    "# bigger the k is longer the running time spending, but the bigger k result output more sparser model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PAMI.frequentPattern.basic import FPGrowth as alg\n",
    "minSup=[3, 6, 9]\n",
    "for sup in minSup:\n",
    "    obj3 = alg.FPGrowth(iFile='td_freq_db_comp_graphics.csv', minSup=sup)\n",
    "    obj3.mine()\n",
    "    frequentPatternsDF_comp_graphics= obj3.getPatternsAsDataFrame()\n",
    "    print('Total No of patterns: ' + str(len(frequentPatternsDF_comp_graphics))) #print the total number of patterns\n",
    "    print('Runtime: ' + str(obj3.getRuntime())) #measure the runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Exercise 18 (take home):\n",
    "Please try to reduce the dimension to 3, and plot the result use 3-D plot. Use at least 3 different angle (camera position) to check your result and describe what you found.\n",
    "\n",
    "$Hint$: you can refer to Axes3D in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "X_pca_3 = PCA(n_components=3).fit_transform(augmented_df.values)\n",
    "X_tsne_3 = TSNE(n_components=3).fit_transform(augmented_df.values)\n",
    "X_umap_3 = umap.UMAP(n_components=3).fit_transform(augmented_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results in subplots\n",
    "col = ['coral', 'blue', 'black', 'orange']\n",
    "categories = X['category_name'].unique() \n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(30, 10))  # Create 3 subplots for PCA, t-SNE, and UMAP\n",
    "ax = plt.axes(projection='3d')\n",
    "fig.suptitle('PCA, t-SNE, and UMAP Comparison')\n",
    "\n",
    "# Define a function to create a scatter plot for each method\n",
    "def plot_scatter(ax, X_reduced, title):\n",
    "    for c, category in zip(col, categories):\n",
    "        xs = X_reduced[X['category_name'] == category].T[0]\n",
    "        ys = X_reduced[X['category_name'] == category].T[1]\n",
    "        zs = X_reduced[X['category_name'] == category].T[2]\n",
    "        ax.scatter3D(xs, ys, zs, c=zs, marker='o', label=category)\n",
    "    \n",
    "    ax.grid(color='gray', linestyle=':', linewidth=2, alpha=0.2)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "# Create scatter plots for PCA, t-SNE, and UMAP\n",
    "plot_scatter(axes[0], X_pca_3, 'PCA')\n",
    "plot_scatter(axes[1], X_tsne_3, 't-SNE')\n",
    "plot_scatter(axes[2], X_umap_3, 'UMAP')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 19 (take home):**\n",
    "Try to generate the binarization using the `category_name` column instead. Does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "\n",
    "# Yes, the category_name column also work too\n",
    "mlb.fit(X.category_name)\n",
    "X['bin_category_name'] = mlb.transform(X['category_name']).tolist()\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **=========Second Part========================================**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>not nostalgia</td>\n",
       "      <td>He was a singer with a golden voice that I lov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nostalgia</td>\n",
       "      <td>The mist beautiful voice ever I listened to hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nostalgia</td>\n",
       "      <td>I have most of Mr. Reeves songs.  Always love ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>not nostalgia</td>\n",
       "      <td>30 day leave from 1st tour in Viet Nam to conv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nostalgia</td>\n",
       "      <td>listening to his songs reminds me of my mum wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment                                            comment\n",
       "0  not nostalgia  He was a singer with a golden voice that I lov...\n",
       "1      nostalgia  The mist beautiful voice ever I listened to hi...\n",
       "2      nostalgia  I have most of Mr. Reeves songs.  Always love ...\n",
       "3  not nostalgia  30 day leave from 1st tour in Viet Nam to conv...\n",
       "4      nostalgia  listening to his songs reminds me of my mum wh..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"hf://datasets/Senem/Nostalgic_Sentiment_Analysis_of_YouTube_Comments_Data/Nostalgic_Sentiment_Analysis_of_YouTube_Comments_Data.csv\")\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
